{"version":3,"sources":["unzip.js","lib/parse.js","lib/PullStream.js","lib/Buffer.js","lib/buffer.js","lib/NoopStream.js","lib/BufferStream.js","lib/parseExtraField.js","lib/parseDateTime.js","lib/parseOne.js","lib/extract.js","lib/Open/index.js","lib/Open/directory.js","lib/Open/unzip.js","lib/Decrypt.js"],"names":[],"mappings":";;;;;;;AAAA;AACA;AACA;ACFA,ADGA;ACFA,ADGA;ACFA,ADGA;AELA,ADGA,ADGA;AELA,ADGA,ADGA;AELA,ADGA,ADGA;AGRA,ADGA,ADGA,ADGA;AGRA,ADGA,ADGA,ADGA;AGRA,ADGA,ADGA;AELA,ADGA,AENA,AHSA;AELA,ADGA,AENA,AHSA;AELA,ADGA,AENA,AHSA;AELA,AENA,AHSA,AENA,AHSA;AELA,AENA,AHSA,AENA,AHSA;AELA,AENA,AHSA,AENA,AHSA;AELA,AGTA,ADGA,AHSA,AENA,AHSA;AELA,AGTA,ADGA,AHSA,AENA,AHSA;AELA,AGTA,ADGA,AHSA,AENA,AHSA;AKdA,ADGA,AHSA,AENA,AHSA,AMlBA;ADIA,ADGA,AHSA,AENA,AHSA,AMlBA;ADIA,ADGA,AHSA,AENA,AHSA,AMlBA;ADIA,ADGA,AHSA,ADGA,AOrBA,ADGA;ADIA,ADGA,AHSA,ADGA,AOrBA,ADGA;ADIA,ADGA,AHSA,ADGA,AOrBA,ADGA;ADIA,ADGA,AHSA,ADGA,AOrBA,ADGA,AENA;AHUA,ADGA,AHSA,ADGA,AOrBA,ADGA,AENA;AHUA,ADGA,AHSA,ADGA,AOrBA,ADGA,AENA;AHUA,ADGA,AHSA,AQxBA,AT2BA,AOrBA,ADGA,AENA;AHUA,ADGA,AHSA,AQxBA,AT2BA,AOrBA,ADGA,AENA;AHUA,ADGA,AHSA,AQxBA,AT2BA,AOrBA,ADGA,AENA;AHUA,ADGA,AMlBA,AT2BA,AQxBA,AT2BA,AOrBA,ADGA,AENA;AHUA,AKfA,AT2BA,AQxBA,AT2BA,AOrBA,ADGA,AENA;AHUA,AKfA,AT2BA,AQxBA,AT2BA,AOrBA,ADGA,AENA;AHUA,AMlBA,ADGA,AT2BA,AQxBA,AT2BA,AOrBA,ADGA,AENA;AHUA,AMlBA,ADGA,AT2BA,AQxBA,AT2BA,AMlBA,AENA;AHUA,AMlBA,ADGA,AT2BA,AQxBA,AT2BA,AMlBA,AENA;AHUA,AMlBA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AMlBA,AENA;AHUA,AMlBA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AMlBA,AENA;AHUA,AMlBA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AMlBA,AENA;AHUA,AQxBA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AMlBA,AENA;AHUA,AQxBA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AMlBA,AENA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AMlBA,AENA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AMlBA,AENA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AMlBA,AENA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AMlBA,AENA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AMlBA,AENA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AMlBA,AENA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AMlBA,AENA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AMlBA,AENA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AMlBA,AENA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AMlBA,AENA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AMlBA,AENA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AMlBA,AENA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AMlBA,AENA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AMlBA,AENA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AMlBA,AENA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,AQxBA,AT2BA,AQxBA;AKdA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AatCA,AFMA,ADGA,AENA,AXiCA,ADGA;AWhCA,ADGA,AENA,AXiCA,ADGA;AWhCA,ADGA,AENA,AXiCA,ADGA;AWhCA,ADGA,AENA,AXiCA,ADGA;AWhCA,ADGA,AENA,AXiCA,ADGA;AWhCA,ADGA,AENA,AXiCA,ADGA;AWhCA,ADGA,AENA,AXiCA,ADGA;AWhCA,ADGA,AENA,AXiCA,ADGA;AWhCA,ADGA,AENA,AXiCA,ADGA;AWhCA,ADGA,AENA,AXiCA,ADGA;AWhCA,ADGA,AENA,AXiCA,ADGA;AWhCA,ADGA,AENA,AXiCA,ADGA;AWhCA,ADGA,AENA,AXiCA,ADGA;AWhCA,ADGA,AENA,AXiCA,ADGA;AWhCA,ADGA,AENA,AXiCA,ADGA;AWhCA,ADGA,AENA,AXiCA,ADGA;AWhCA,ADGA,AENA,AXiCA,ADGA;AWhCA,ADGA,AENA,AXiCA,ADGA;AWhCA,ACHA,AXiCA,ADGA;AWhCA,ACHA,AXiCA,ADGA;AWhCA,ACHA,AXiCA,ADGA;AWhCA,ACHA,AXiCA,ADGA;AWhCA,ACHA,AXiCA,ADGA;AWhCA,ACHA,AXiCA,ADGA;AWhCA,ACHA,AXiCA,ADGA;AWhCA,ACHA,AXiCA,ADGA;AWhCA,ACHA,AXiCA,ADGA;AWhCA,ACHA,AXiCA,ADGA;AWhCA,ACHA,AXiCA,ADGA;AWhCA,ACHA,AXiCA,ADGA;AWhCA,ACHA,AXiCA,ADGA;AWhCA,ACHA,AXiCA,ADGA;AWhCA,ACHA,AXiCA,ADGA;AWhCA,ACHA,AXiCA,ADGA;AWhCA,ACHA,AXiCA,ADGA;AWhCA,ACHA,AXiCA,ADGA;AWhCA,ACHA,AXiCA,ADGA;AWhCA,ACHA,AXiCA,ADGA;AWhCA,ACHA,AXiCA,ADGA;AWhCA,ACHA,AZoCA;AWhCA,ACHA,AZoCA;AWhCA,ACHA,AZoCA;AWhCA,ACHA,AZoCA;AWhCA,ACHA,AZoCA;AWhCA,ACHA,AZoCA;AWhCA,ACHA,AZoCA;AWhCA,ACHA,AZoCA;AWhCA,ACHA,AZoCA;AWhCA,ACHA,AZoCA;AWhCA,ACHA,AZoCA;AWhCA,ACHA,AZoCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AWhCA,AXiCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","file":"index.js","sourcesContent":["\n// Polyfills for node 0.8\nrequire('listenercount');\nrequire('buffer-indexof-polyfill');\nrequire('setimmediate');\n\n\nexports.Parse = require('./lib/parse');\nexports.ParseOne = require('./lib/parseOne');\nexports.Extract = require('./lib/extract');\nexports.Open = require('./lib/Open');","var util = require('util');\nvar zlib = require('zlib');\nvar Stream = require('stream');\nvar binary = require('binary');\nvar Promise = require('bluebird');\nvar PullStream = require('./PullStream');\nvar NoopStream = require('./NoopStream');\nvar BufferStream = require('./BufferStream');\nvar parseExtraField = require('./parseExtraField');\nvar Buffer = require('./Buffer');\nvar parseDateTime = require('./parseDateTime');\n\n// Backwards compatibility for node versions < 8\nif (!Stream.Writable || !Stream.Writable.prototype.destroy)\n  Stream = require('readable-stream');\n\nvar endDirectorySignature = Buffer.alloc(4);\nendDirectorySignature.writeUInt32LE(0x06054b50, 0);\n\nfunction Parse(opts) {\n  if (!(this instanceof Parse)) {\n    return new Parse(opts);\n  }\n  var self = this;\n  self._opts = opts || { verbose: false };\n\n  PullStream.call(self, self._opts);\n  self.on('finish',function() {\n    self.emit('close');\n  });\n  self._readRecord().catch(function(e) {\n    if (!self.__emittedError || self.__emittedError !== e)\n      self.emit('error',e);\n  });\n}\n\nutil.inherits(Parse, PullStream);\n\nParse.prototype._readRecord = function () {\n  var self = this;\n  return self.pull(4).then(function(data) {\n    if (data.length === 0)\n      return;\n\n    var signature = data.readUInt32LE(0);\n\n    if (signature === 0x34327243) {\n      return self._readCrxHeader();\n    }\n    if (signature === 0x04034b50) {\n      return self._readFile();\n    }\n    else if (signature === 0x02014b50) {\n      self.__ended = true;\n      return self._readCentralDirectoryFileHeader();\n    }\n    else if (signature === 0x06054b50) {\n      return self._readEndOfCentralDirectoryRecord();\n    }\n    else if (self.__ended) {\n      return self.pull(endDirectorySignature).then(function() {\n          return self._readEndOfCentralDirectoryRecord();\n        });\n    }\n    else\n      self.emit('error', new Error('invalid signature: 0x' + signature.toString(16)));\n  });\n};\n\nParse.prototype._readCrxHeader = function() {\n  var self = this;\n  return self.pull(12).then(function(data) {\n    self.crxHeader = binary.parse(data)\n      .word32lu('version')\n      .word32lu('pubKeyLength')\n      .word32lu('signatureLength')\n      .vars;\n    return self.pull(self.crxHeader.pubKeyLength + self.crxHeader.signatureLength);\n  }).then(function(data) {\n    self.crxHeader.publicKey = data.slice(0,self.crxHeader.pubKeyLength);\n    self.crxHeader.signature = data.slice(self.crxHeader.pubKeyLength);\n    self.emit('crx-header',self.crxHeader);\n    return self._readRecord();\n  });\n};\n\nParse.prototype._readFile = function () {\n  var self = this;\n  return self.pull(26).then(function(data) {\n    var vars = binary.parse(data)\n      .word16lu('versionsNeededToExtract')\n      .word16lu('flags')\n      .word16lu('compressionMethod')\n      .word16lu('lastModifiedTime')\n      .word16lu('lastModifiedDate')\n      .word32lu('crc32')\n      .word32lu('compressedSize')\n      .word32lu('uncompressedSize')\n      .word16lu('fileNameLength')\n      .word16lu('extraFieldLength')\n      .vars;\n\n    vars.lastModifiedDateTime = parseDateTime(vars.lastModifiedDate, vars.lastModifiedTime);\n\n    if (self.crxHeader) vars.crxHeader = self.crxHeader;\n\n    return self.pull(vars.fileNameLength).then(function(fileNameBuffer) {\n      var fileName = fileNameBuffer.toString('utf8');\n      var entry = Stream.PassThrough();\n      var __autodraining = false;\n\n      entry.autodrain = function() {\n        __autodraining = true;\n        var draining = entry.pipe(NoopStream());\n        draining.promise = function() {\n          return new Promise(function(resolve, reject) {\n            draining.on('finish',resolve);\n            draining.on('error',reject);\n          });\n        };\n        return draining;\n      };\n\n      entry.buffer = function() {\n        return BufferStream(entry);\n      };\n\n      entry.path = fileName;\n      entry.props = {};\n      entry.props.path = fileName;\n      entry.props.pathBuffer = fileNameBuffer;\n      entry.props.flags = {\n        \"isUnicode\": vars.flags & 0x11\n      };\n      entry.type = (vars.uncompressedSize === 0 && /[\\/\\\\]$/.test(fileName)) ? 'Directory' : 'File';\n\n      if (self._opts.verbose) {\n        if (entry.type === 'Directory') {\n          console.log('   creating:', fileName);\n        } else if (entry.type === 'File') {\n          if (vars.compressionMethod === 0) {\n            console.log(' extracting:', fileName);\n          } else {\n            console.log('  inflating:', fileName);\n          }\n        }\n      }\n\n      return self.pull(vars.extraFieldLength).then(function(extraField) {\n        var extra = parseExtraField(extraField, vars);\n\n        entry.vars = vars;\n        entry.extra = extra;\n\n        if (self._opts.forceStream) {\n          self.push(entry);\n        } else {\n          self.emit('entry', entry);\n\n          if (self._readableState.pipesCount || (self._readableState.pipes && self._readableState.pipes.length))\n            self.push(entry);\n        }\n\n        if (self._opts.verbose)\n          console.log({\n            filename:fileName,\n            vars: vars,\n            extra: extra\n          });\n\n        var fileSizeKnown = !(vars.flags & 0x08) || vars.compressedSize > 0,\n            eof;\n\n        entry.__autodraining = __autodraining;  // expose __autodraining for test purposes\n        var inflater = (vars.compressionMethod && !__autodraining) ? zlib.createInflateRaw() : Stream.PassThrough();\n\n        if (fileSizeKnown) {\n          entry.size = vars.uncompressedSize;\n          eof = vars.compressedSize;\n        } else {\n          eof = Buffer.alloc(4);\n          eof.writeUInt32LE(0x08074b50, 0);\n        }\n\n        return new Promise(function(resolve, reject) {\n          self.stream(eof)\n            .pipe(inflater)\n            .on('error',function(err) { self.emit('error',err);})\n            .pipe(entry)\n            .on('finish', function() {\n              return fileSizeKnown ?\n                self._readRecord().then(resolve).catch(reject) :\n                self._processDataDescriptor(entry).then(resolve).catch(reject);\n            });\n        });\n      });\n    });\n  });\n};\n\nParse.prototype._processDataDescriptor = function (entry) {\n  var self = this;\n  return self.pull(16).then(function(data) {\n    var vars = binary.parse(data)\n      .word32lu('dataDescriptorSignature')\n      .word32lu('crc32')\n      .word32lu('compressedSize')\n      .word32lu('uncompressedSize')\n      .vars;\n\n    entry.size = vars.uncompressedSize;\n    return self._readRecord();\n  });\n};\n\nParse.prototype._readCentralDirectoryFileHeader = function () {\n  var self = this;\n  return self.pull(42).then(function(data) {\n\n    var vars = binary.parse(data)\n      .word16lu('versionMadeBy')\n      .word16lu('versionsNeededToExtract')\n      .word16lu('flags')\n      .word16lu('compressionMethod')\n      .word16lu('lastModifiedTime')\n      .word16lu('lastModifiedDate')\n      .word32lu('crc32')\n      .word32lu('compressedSize')\n      .word32lu('uncompressedSize')\n      .word16lu('fileNameLength')\n      .word16lu('extraFieldLength')\n      .word16lu('fileCommentLength')\n      .word16lu('diskNumber')\n      .word16lu('internalFileAttributes')\n      .word32lu('externalFileAttributes')\n      .word32lu('offsetToLocalFileHeader')\n      .vars;\n\n    return self.pull(vars.fileNameLength).then(function(fileName) {\n      vars.fileName = fileName.toString('utf8');\n      return self.pull(vars.extraFieldLength);\n    })\n    .then(function(extraField) {\n      return self.pull(vars.fileCommentLength);\n    })\n    .then(function(fileComment) {\n      return self._readRecord();\n    });\n  });\n};\n\nParse.prototype._readEndOfCentralDirectoryRecord = function() {\n  var self = this;\n  return self.pull(18).then(function(data) {\n\n    var vars = binary.parse(data)\n      .word16lu('diskNumber')\n      .word16lu('diskStart')\n      .word16lu('numberOfRecordsOnDisk')\n      .word16lu('numberOfRecords')\n      .word32lu('sizeOfCentralDirectory')\n      .word32lu('offsetToStartOfCentralDirectory')\n      .word16lu('commentLength')\n      .vars;\n\n    return self.pull(vars.commentLength).then(function(comment) {\n      comment = comment.toString('utf8');\n      self.end();\n      self.push(null);\n    });\n\n  });\n};\n\nParse.prototype.promise = function() {\n  var self = this;\n  return new Promise(function(resolve,reject) {\n    self.on('finish',resolve);\n    self.on('error',reject);\n  });\n};\n\nmodule.exports = Parse;\n","var Stream = require('stream');\nvar Promise = require('bluebird');\nvar util = require('util');\nvar Buffer = require('./Buffer');\nvar strFunction = 'function';\n\n// Backwards compatibility for node versions < 8\nif (!Stream.Writable || !Stream.Writable.prototype.destroy)\n  Stream = require('readable-stream');\n\nfunction PullStream() {\n  if (!(this instanceof PullStream))\n    return new PullStream();\n\n  Stream.Duplex.call(this,{decodeStrings:false, objectMode:true});\n  this.buffer = Buffer.from('');\n  var self = this;\n  self.on('finish',function() {\n    self.finished = true;\n    self.emit('chunk',false);\n  });\n}\n\nutil.inherits(PullStream,Stream.Duplex);\n\nPullStream.prototype._write = function(chunk,e,cb) {\n  this.buffer = Buffer.concat([this.buffer,chunk]);\n  this.cb = cb;\n  this.emit('chunk');\n};\n\n\n// The `eof` parameter is interpreted as `file_length` if the type is number\n// otherwise (i.e. buffer) it is interpreted as a pattern signaling end of stream\nPullStream.prototype.stream = function(eof,includeEof) {\n  var p = Stream.PassThrough();\n  var done,self= this;\n\n  function cb() {\n    if (typeof self.cb === strFunction) {\n      var callback = self.cb;\n      self.cb = undefined;\n      return callback();\n    }\n  }\n\n  function pull() {\n    var packet;\n    if (self.buffer && self.buffer.length) {\n      if (typeof eof === 'number') {\n        packet = self.buffer.slice(0,eof);\n        self.buffer = self.buffer.slice(eof);\n        eof -= packet.length;\n        done = !eof;\n      } else {\n        var match = self.buffer.indexOf(eof);\n        if (match !== -1) {\n          // store signature match byte offset to allow us to reference\n          // this for zip64 offset\n          self.match = match\n          if (includeEof) match = match + eof.length;\n          packet = self.buffer.slice(0,match);\n          self.buffer = self.buffer.slice(match);\n          done = true;\n        } else {\n          var len = self.buffer.length - eof.length;\n          if (len <= 0) {\n            cb();\n          } else {\n            packet = self.buffer.slice(0,len);\n            self.buffer = self.buffer.slice(len);\n          }\n        }\n      }\n      if (packet) p.write(packet,function() {\n        if (self.buffer.length === 0 || (eof.length && self.buffer.length <= eof.length)) cb();\n      });\n    }\n    \n    if (!done) {\n      if (self.finished && !this.__ended) {\n        self.removeListener('chunk',pull);\n        self.emit('error', new Error('FILE_ENDED'));\n        this.__ended = true;\n        return;\n      }\n      \n    } else {\n      self.removeListener('chunk',pull);\n      p.end();\n    }\n  }\n\n  self.on('chunk',pull);\n  pull();\n  return p;\n};\n\nPullStream.prototype.pull = function(eof,includeEof) {\n  if (eof === 0) return Promise.resolve('');\n\n  // If we already have the required data in buffer\n  // we can resolve the request immediately\n  if (!isNaN(eof) && this.buffer.length > eof) {\n    var data = this.buffer.slice(0,eof);\n    this.buffer = this.buffer.slice(eof);\n    return Promise.resolve(data);\n  }\n\n  // Otherwise we stream until we have it\n  var buffer = Buffer.from(''),\n      self = this;\n\n  var concatStream = Stream.Transform();\n  concatStream._transform = function(d,e,cb) {\n    buffer = Buffer.concat([buffer,d]);\n    cb();\n  };\n  \n  var rejectHandler;\n  var pullStreamRejectHandler;\n  return new Promise(function(resolve,reject) {\n    rejectHandler = reject;\n    pullStreamRejectHandler = function(e) {\n      self.__emittedError = e;\n      reject(e);\n    }\n    if (self.finished)\n      return reject(new Error('FILE_ENDED'));\n    self.once('error',pullStreamRejectHandler);  // reject any errors from pullstream itself\n    self.stream(eof,includeEof)\n      .on('error',reject)\n      .pipe(concatStream)\n      .on('finish',function() {resolve(buffer);})\n      .on('error',reject);\n  })\n  .finally(function() {\n    self.removeListener('error',rejectHandler);\n    self.removeListener('error',pullStreamRejectHandler);\n  });\n};\n\nPullStream.prototype._read = function(){};\n\nmodule.exports = PullStream;\n","var Buffer = require('buffer').Buffer;\n\n// Backwards compatibility for node versions < 8\nif (Buffer.from === undefined) {\n  Buffer.from = function (a, b, c) {\n    return new Buffer(a, b, c)\n  };\n\n  Buffer.alloc = Buffer.from;\n}\n\nmodule.exports = Buffer;","var Buffer = require('buffer').Buffer;\n\n// Backwards compatibility for node versions < 8\nif (Buffer.from === undefined) {\n  Buffer.from = function (a, b, c) {\n    return new Buffer(a, b, c)\n  };\n\n  Buffer.alloc = Buffer.from;\n}\n\nmodule.exports = Buffer;","var Stream = require('stream');\nvar util = require('util');\n\n// Backwards compatibility for node versions < 8\nif (!Stream.Writable || !Stream.Writable.prototype.destroy)\n  Stream = require('readable-stream');\n\nfunction NoopStream() {\n  if (!(this instanceof NoopStream)) {\n    return new NoopStream();\n  }\n  Stream.Transform.call(this);\n}\n\nutil.inherits(NoopStream,Stream.Transform);\n\nNoopStream.prototype._transform = function(d,e,cb) { cb() ;};\n  \nmodule.exports = NoopStream;","var Promise = require('bluebird');\nvar Stream = require('stream');\nvar Buffer = require('./Buffer');\n\n// Backwards compatibility for node versions < 8\nif (!Stream.Writable || !Stream.Writable.prototype.destroy)\n  Stream = require('readable-stream');\n\nmodule.exports = function(entry) {\n  return new Promise(function(resolve,reject) {\n    var chunks = [];\n    var bufferStream = Stream.Transform()\n      .on('finish',function() {\n        resolve(Buffer.concat(chunks));\n      })\n      .on('error',reject);\n        \n    bufferStream._transform = function(d,e,cb) {\n      chunks.push(d);\n      cb();\n    };\n    entry.on('error',reject)\n      .pipe(bufferStream);\n  });\n};\n","var binary = require('binary');\n\nmodule.exports = function(extraField, vars) {\n  var extra;\n  // Find the ZIP64 header, if present.\n  while(!extra && extraField && extraField.length) {\n    var candidateExtra = binary.parse(extraField)\n      .word16lu('signature')\n      .word16lu('partsize')\n      .word64lu('uncompressedSize')\n      .word64lu('compressedSize')\n      .word64lu('offset')\n      .word64lu('disknum')\n      .vars;\n\n    if(candidateExtra.signature === 0x0001) {\n      extra = candidateExtra;\n    } else {\n      // Advance the buffer to the next part.\n      // The total size of this part is the 4 byte header + partsize.\n      extraField = extraField.slice(candidateExtra.partsize + 4);\n    }\n  }\n\n  extra = extra || {};\n\n  if (vars.compressedSize === 0xffffffff)\n    vars.compressedSize = extra.compressedSize;\n\n  if (vars.uncompressedSize  === 0xffffffff)\n    vars.uncompressedSize= extra.uncompressedSize;\n\n  if (vars.offsetToLocalFileHeader === 0xffffffff)\n    vars.offsetToLocalFileHeader= extra.offset;\n\n  return extra;\n};\n","// Dates in zip file entries are stored as DosDateTime\n// Spec is here: https://docs.microsoft.com/en-us/windows/win32/api/winbase/nf-winbase-dosdatetimetofiletime\n\nmodule.exports = function parseDateTime(date, time) {\n  const day = date & 0x1F;\n  const month = date >> 5 & 0x0F;\n  const year = (date >> 9 & 0x7F) + 1980;\n  const seconds = time ? (time & 0x1F) * 2 : 0;\n  const minutes = time ? (time >> 5) & 0x3F : 0;\n  const hours = time ? (time >> 11): 0;\n\n  return new Date(Date.UTC(year, month-1, day, hours, minutes, seconds));\n};","var Stream = require('stream');\nvar Parse = require('./parse');\nvar duplexer2 = require('duplexer2');\nvar BufferStream = require('./BufferStream');\n\n// Backwards compatibility for node versions < 8\nif (!Stream.Writable || !Stream.Writable.prototype.destroy)\n  Stream = require('readable-stream');\n\nfunction parseOne(match,opts) {\n  var inStream = Stream.PassThrough({objectMode:true});\n  var outStream = Stream.PassThrough();\n  var transform = Stream.Transform({objectMode:true});\n  var re = match instanceof RegExp ? match : (match && new RegExp(match));\n  var found;\n\n  transform._transform = function(entry,e,cb) {\n    if (found || (re && !re.exec(entry.path))) {\n      entry.autodrain();\n      return cb();\n    } else {\n      found = true;\n      out.emit('entry',entry);\n      entry.on('error',function(e) {\n        outStream.emit('error',e);\n      });\n      entry.pipe(outStream)\n        .on('error',function(err) {\n          cb(err);\n        })\n        .on('finish',function(d) {\n          cb(null,d);\n        });\n    }\n  };\n\n  inStream.pipe(Parse(opts))\n    .on('error',function(err) {\n      outStream.emit('error',err);\n    })\n    .pipe(transform)\n    .on('error',Object)  // Silence error as its already addressed in transform\n    .on('finish',function() {\n      if (!found)\n        outStream.emit('error',new Error('PATTERN_NOT_FOUND'));\n      else\n        outStream.end();\n    });\n\n  var out = duplexer2(inStream,outStream);\n  out.buffer = function() {\n    return BufferStream(outStream);\n  };\n\n  return out;\n}\n\nmodule.exports = parseOne;\n","module.exports = Extract;\n\nvar Parse = require('./parse');\nvar Writer = require('fstream').Writer;\nvar path = require('path');\nvar stream = require('stream');\nvar duplexer2 = require('duplexer2');\nvar Promise = require('bluebird');\n\nfunction Extract (opts) {\n  // make sure path is normalized before using it\n  opts.path = path.resolve(path.normalize(opts.path));\n\n  var parser = new Parse(opts);\n\n  var outStream = new stream.Writable({objectMode: true});\n  outStream._write = function(entry, encoding, cb) {\n\n    if (entry.type == 'Directory') return cb();\n\n    // to avoid zip slip (writing outside of the destination), we resolve\n    // the target path, and make sure it's nested in the intended\n    // destination, or not extract it otherwise.\n    var extractPath = path.join(opts.path, entry.path);\n    if (extractPath.indexOf(opts.path) != 0) {\n      return cb();\n    }\n\n    const writer = opts.getWriter ? opts.getWriter({path: extractPath}) :  Writer({ path: extractPath });\n\n    entry.pipe(writer)\n      .on('error', cb)\n      .on('close', cb);\n  };\n\n  var extract = duplexer2(parser,outStream);\n  parser.once('crx-header', function(crxHeader) {\n    extract.crxHeader = crxHeader;\n  });\n\n  parser\n    .pipe(outStream)\n    .on('finish',function() {\n      extract.emit('close');\n    });\n  \n  extract.promise = function() {\n    return new Promise(function(resolve, reject) {\n      extract.on('close', resolve);\n      extract.on('error',reject);\n    });\n  };\n\n  return extract;\n}\n","var fs = require('graceful-fs');\nvar Promise = require('bluebird');\nvar directory = require('./directory');\nvar Stream = require('stream');\n\n// Backwards compatibility for node versions < 8\nif (!Stream.Writable || !Stream.Writable.prototype.destroy)\n  Stream = require('readable-stream');\n\nmodule.exports = {\n  buffer: function(buffer, options) {\n    var source = {\n      stream: function(offset, length) {\n        var stream = Stream.PassThrough();\n        stream.end(buffer.slice(offset, length));\n        return stream;\n      },\n      size: function() {\n        return Promise.resolve(buffer.length);\n      }\n    };\n    return directory(source, options);\n  },\n  file: function(filename, options) {\n    var source = {\n      stream: function(offset,length) {\n        return fs.createReadStream(filename,{start: offset, end: length && offset+length});\n      },\n      size: function() {\n        return new Promise(function(resolve,reject) {\n          fs.stat(filename,function(err,d) {\n            if (err)\n              reject(err);\n            else\n              resolve(d.size);\n          });\n        });\n      }\n    };\n    return directory(source, options);\n  },\n\n  url: function(request, params, options) {\n    if (typeof params === 'string')\n      params = {url: params};\n    if (!params.url)\n      throw 'URL missing';\n    params.headers = params.headers || {};\n\n    var source = {\n      stream : function(offset,length) {\n        var options = Object.create(params);\n        options.headers = Object.create(params.headers);\n        options.headers.range = 'bytes='+offset+'-' + (length ? length : '');\n        return request(options);\n      },\n      size: function() {\n        return new Promise(function(resolve,reject) {\n          var req = request(params);\n          req.on('response',function(d) {\n            req.abort();\n            if (!d.headers['content-length'])\n              reject(new Error('Missing content length header'));\n            else\n              resolve(d.headers['content-length']);\n          }).on('error',reject);\n        });\n      }\n    };\n\n    return directory(source, options);\n  },\n\n  s3 : function(client,params, options) {\n    var source = {\n      size: function() {\n        return new Promise(function(resolve,reject) {\n          client.headObject(params, function(err,d) {\n            if (err)\n              reject(err);\n            else\n              resolve(d.ContentLength);\n          });\n        });\n      },\n      stream: function(offset,length) {\n        var d = {};\n        for (var key in params)\n          d[key] = params[key];\n        d.Range = 'bytes='+offset+'-' + (length ? length : '');\n        return client.getObject(d).createReadStream();\n      }\n    };\n\n    return directory(source, options);\n  }\n};\n","var binary = require('binary');\nvar PullStream = require('../PullStream');\nvar unzip = require('./unzip');\nvar Promise = require('bluebird');\nvar BufferStream = require('../BufferStream');\nvar parseExtraField = require('../parseExtraField');\nvar Buffer = require('../Buffer');\nvar path = require('path');\nvar Writer = require('fstream').Writer;\nvar parseDateTime = require('../parseDateTime');\n\nvar signature = Buffer.alloc(4);\nsignature.writeUInt32LE(0x06054b50,0);\n\nfunction getCrxHeader(source) {\n  var sourceStream = source.stream(0).pipe(PullStream());\n\n  return sourceStream.pull(4).then(function(data) {\n    var signature = data.readUInt32LE(0);\n    if (signature === 0x34327243) {\n      var crxHeader;\n      return sourceStream.pull(12).then(function(data) {\n        crxHeader = binary.parse(data)\n          .word32lu('version')\n          .word32lu('pubKeyLength')\n          .word32lu('signatureLength')\n          .vars;\n      }).then(function() {\n        return sourceStream.pull(crxHeader.pubKeyLength +crxHeader.signatureLength);\n      }).then(function(data) {\n        crxHeader.publicKey = data.slice(0,crxHeader.pubKeyLength);\n        crxHeader.signature = data.slice(crxHeader.pubKeyLength);\n        crxHeader.size = 16 + crxHeader.pubKeyLength +crxHeader.signatureLength;\n        return crxHeader;\n      });\n    }\n  });\n}\n\n// Zip64 File Format Notes: https://pkware.cachefly.net/webdocs/casestudies/APPNOTE.TXT\nfunction getZip64CentralDirectory(source, zip64CDL) {\n  var d64loc = binary.parse(zip64CDL)\n    .word32lu('signature')\n    .word32lu('diskNumber')\n    .word64lu('offsetToStartOfCentralDirectory')\n    .word32lu('numberOfDisks')\n    .vars;\n\n  if (d64loc.signature != 0x07064b50) {\n    throw new Error('invalid zip64 end of central dir locator signature (0x07064b50): 0x' + d64loc.signature.toString(16));\n  }\n\n  var dir64 = PullStream();\n  source.stream(d64loc.offsetToStartOfCentralDirectory).pipe(dir64);\n\n  return dir64.pull(56)\n}\n\n// Zip64 File Format Notes: https://pkware.cachefly.net/webdocs/casestudies/APPNOTE.TXT\nfunction parseZip64DirRecord (dir64record) {\n  var vars = binary.parse(dir64record)\n    .word32lu('signature')\n    .word64lu('sizeOfCentralDirectory')\n    .word16lu('version')\n    .word16lu('versionsNeededToExtract')\n    .word32lu('diskNumber')\n    .word32lu('diskStart')\n    .word64lu('numberOfRecordsOnDisk')\n    .word64lu('numberOfRecords')\n    .word64lu('sizeOfCentralDirectory')\n    .word64lu('offsetToStartOfCentralDirectory')\n    .vars;\n\n  if (vars.signature != 0x06064b50) {\n    throw new Error('invalid zip64 end of central dir locator signature (0x06064b50): 0x0' + vars.signature.toString(16));\n  }\n\n  return vars\n}\n\nmodule.exports = function centralDirectory(source, options) {\n  var endDir = PullStream(),\n      records = PullStream(),\n      tailSize = (options && options.tailSize) || 80,\n      sourceSize,\n      crxHeader,\n      startOffset,\n      vars;\n\n  if (options && options.crx)\n    crxHeader = getCrxHeader(source);\n\n  return source.size()\n    .then(function(size) {\n      sourceSize = size;\n\n      source.stream(Math.max(0,size-tailSize))\n        .on('error', function (error) { endDir.emit('error', error) })\n        .pipe(endDir);\n\n      return endDir.pull(signature);\n    })\n    .then(function() {\n      return Promise.props({directory: endDir.pull(22), crxHeader: crxHeader});\n    })\n    .then(function(d) {\n      var data = d.directory;\n      startOffset = d.crxHeader && d.crxHeader.size || 0;\n\n      vars = binary.parse(data)\n        .word32lu('signature')\n        .word16lu('diskNumber')\n        .word16lu('diskStart')\n        .word16lu('numberOfRecordsOnDisk')\n        .word16lu('numberOfRecords')\n        .word32lu('sizeOfCentralDirectory')\n        .word32lu('offsetToStartOfCentralDirectory')\n        .word16lu('commentLength')\n        .vars;\n\n      // Is this zip file using zip64 format? Use same check as Go:\n      // https://github.com/golang/go/blob/master/src/archive/zip/reader.go#L503\n      // For zip64 files, need to find zip64 central directory locator header to extract\n      // relative offset for zip64 central directory record.\n      if (vars.numberOfRecords == 0xffff|| vars.numberOfRecords == 0xffff ||\n        vars.offsetToStartOfCentralDirectory == 0xffffffff) {\n\n        // Offset to zip64 CDL is 20 bytes before normal CDR\n        const zip64CDLSize = 20\n        const zip64CDLOffset = sourceSize - (tailSize - endDir.match + zip64CDLSize)\n        const zip64CDLStream = PullStream();\n\n        source.stream(zip64CDLOffset).pipe(zip64CDLStream);\n\n        return zip64CDLStream.pull(zip64CDLSize)\n          .then(function (d) { return getZip64CentralDirectory(source, d) })\n          .then(function (dir64record) {\n            vars = parseZip64DirRecord(dir64record)\n          })\n      } else {\n        vars.offsetToStartOfCentralDirectory += startOffset;\n      }\n    })\n    .then(function() {\n      source.stream(vars.offsetToStartOfCentralDirectory).pipe(records);\n\n      vars.extract = function(opts) {\n        if (!opts || !opts.path) throw new Error('PATH_MISSING');\n        return vars.files.then(function(files) {\n          return Promise.map(files, function(entry) {\n            if (entry.type == 'Directory') return;\n\n            // to avoid zip slip (writing outside of the destination), we resolve\n            // the target path, and make sure it's nested in the intended\n            // destination, or not extract it otherwise.\n            var extractPath = path.join(opts.path, entry.path);\n            if (extractPath.indexOf(opts.path) != 0) {\n              return;\n            }\n            var writer = opts.getWriter ? opts.getWriter({path: extractPath}) :  Writer({ path: extractPath });\n\n            return new Promise(function(resolve, reject) {\n              entry.stream(opts.password)\n                .on('error',reject)\n                .pipe(writer)\n                .on('close',resolve)\n                .on('error',reject);\n            });\n          }, opts.concurrency > 1 ? {concurrency: opts.concurrency || undefined} : undefined);\n        });\n      };\n\n      vars.files = Promise.mapSeries(Array(vars.numberOfRecords),function() {\n        return records.pull(46).then(function(data) {    \n          var vars = binary.parse(data)\n            .word32lu('signature')\n            .word16lu('versionMadeBy')\n            .word16lu('versionsNeededToExtract')\n            .word16lu('flags')\n            .word16lu('compressionMethod')\n            .word16lu('lastModifiedTime')\n            .word16lu('lastModifiedDate')\n            .word32lu('crc32')\n            .word32lu('compressedSize')\n            .word32lu('uncompressedSize')\n            .word16lu('fileNameLength')\n            .word16lu('extraFieldLength')\n            .word16lu('fileCommentLength')\n            .word16lu('diskNumber')\n            .word16lu('internalFileAttributes')\n            .word32lu('externalFileAttributes')\n            .word32lu('offsetToLocalFileHeader')\n            .vars;\n\n        vars.offsetToLocalFileHeader += startOffset;\n        vars.lastModifiedDateTime = parseDateTime(vars.lastModifiedDate, vars.lastModifiedTime);\n\n        return records.pull(vars.fileNameLength).then(function(fileNameBuffer) {\n          vars.pathBuffer = fileNameBuffer;\n          vars.path = fileNameBuffer.toString('utf8');\n          vars.isUnicode = vars.flags & 0x11;\n          return records.pull(vars.extraFieldLength);\n        })\n        .then(function(extraField) {\n          vars.extra = parseExtraField(extraField, vars);\n          return records.pull(vars.fileCommentLength);\n        })\n        .then(function(comment) {\n          vars.comment = comment;\n          vars.type = (vars.uncompressedSize === 0 && /[\\/\\\\]$/.test(vars.path)) ? 'Directory' : 'File';\n          vars.stream = function(_password) {\n            return unzip(source, vars.offsetToLocalFileHeader,_password, vars);\n          };\n          vars.buffer = function(_password) {\n            return BufferStream(vars.stream(_password));\n          };\n          return vars;\n        });\n      });\n    });\n\n    return Promise.props(vars);\n  });\n};\n","var Promise = require('bluebird');\nvar Decrypt = require('../Decrypt');\nvar PullStream = require('../PullStream');\nvar Stream = require('stream');\nvar binary = require('binary');\nvar zlib = require('zlib');\nvar parseExtraField = require('../parseExtraField');\nvar Buffer = require('../Buffer');\nvar parseDateTime = require('../parseDateTime');\n\n// Backwards compatibility for node versions < 8\nif (!Stream.Writable || !Stream.Writable.prototype.destroy)\n  Stream = require('readable-stream');\n\nmodule.exports = function unzip(source,offset,_password, directoryVars) {\n  var file = PullStream(),\n      entry = Stream.PassThrough();\n\n  var req = source.stream(offset);\n  req.pipe(file).on('error', function(e) {\n    entry.emit('error', e);\n  });\n\n  entry.vars = file.pull(30)\n    .then(function(data) {\n      var vars = binary.parse(data)\n        .word32lu('signature')\n        .word16lu('versionsNeededToExtract')\n        .word16lu('flags')\n        .word16lu('compressionMethod')\n        .word16lu('lastModifiedTime')\n        .word16lu('lastModifiedDate')\n        .word32lu('crc32')\n        .word32lu('compressedSize')\n        .word32lu('uncompressedSize')\n        .word16lu('fileNameLength')\n        .word16lu('extraFieldLength')\n        .vars;\n\n      vars.lastModifiedDateTime = parseDateTime(vars.lastModifiedDate, vars.lastModifiedTime);\n\n      return file.pull(vars.fileNameLength)\n        .then(function(fileName) {\n          vars.fileName = fileName.toString('utf8');\n          return file.pull(vars.extraFieldLength);\n        })\n        .then(function(extraField) {\n          var checkEncryption;\n          vars.extra = parseExtraField(extraField, vars);\n          // Ignore logal file header vars if the directory vars are available\n          if (directoryVars && directoryVars.compressedSize) vars = directoryVars;\n\n          if (vars.flags & 0x01) checkEncryption = file.pull(12)\n            .then(function(header) {\n              if (!_password)\n                throw new Error('MISSING_PASSWORD');\n\n              var decrypt = Decrypt();\n\n              String(_password).split('').forEach(function(d) {\n                decrypt.update(d);\n              });\n\n              for (var i=0; i < header.length; i++)\n                header[i] = decrypt.decryptByte(header[i]);\n\n              vars.decrypt = decrypt;\n              vars.compressedSize -= 12;\n\n              var check = (vars.flags & 0x8) ? (vars.lastModifiedTime >> 8) & 0xff : (vars.crc32 >> 24) & 0xff;\n              if (header[11] !== check)\n                throw new Error('BAD_PASSWORD');\n\n              return vars;\n            });\n\n          return Promise.resolve(checkEncryption)\n            .then(function() {\n              entry.emit('vars',vars);\n              return vars;\n            });\n        });\n    });\n\n    entry.vars.then(function(vars) {\n      var fileSizeKnown = !(vars.flags & 0x08) || vars.compressedSize > 0,\n          eof;\n\n      var inflater = vars.compressionMethod ? zlib.createInflateRaw() : Stream.PassThrough();\n\n      if (fileSizeKnown) {\n        entry.size = vars.uncompressedSize;\n        eof = vars.compressedSize;\n      } else {\n        eof = Buffer.alloc(4);\n        eof.writeUInt32LE(0x08074b50, 0);\n      }\n\n      var stream = file.stream(eof);\n\n      if (vars.decrypt)\n        stream = stream.pipe(vars.decrypt.stream());\n\n      stream\n        .pipe(inflater)\n        .on('error',function(err) { entry.emit('error',err);})\n        .pipe(entry)\n        .on('finish', function() {\n          if (req.abort)\n            req.abort();\n          else if (req.close)\n            req.close();\n          else if (req.push)\n            req.push();\n          else\n            console.log('warning - unable to close stream');\n        });\n    })\n    .catch(function(e) {\n      entry.emit('error',e);\n    });\n\n  return entry;\n};\n","var bigInt = require('big-integer');\nvar Stream = require('stream');\n\n// Backwards compatibility for node versions < 8\nif (!Stream.Writable || !Stream.Writable.prototype.destroy)\n  Stream = require('readable-stream');\n\nvar table;\n\nfunction generateTable() {\n  var poly = 0xEDB88320,c,n,k;\n  table = [];\n  for (n = 0; n < 256; n++) {\n    c = n;\n    for (k = 0; k < 8; k++)\n      c = (c & 1) ? poly ^ (c >>> 1) :  c = c >>> 1;\n    table[n] = c >>> 0;\n  }\n}\n\nfunction crc(ch,crc) {\n  if (!table)\n    generateTable();\n\n  if (ch.charCodeAt)\n    ch = ch.charCodeAt(0);        \n\n  return (bigInt(crc).shiftRight(8).and(0xffffff)).xor(table[bigInt(crc).xor(ch).and(0xff)]).value;\n}\n\nfunction Decrypt() {\n  if (!(this instanceof Decrypt))\n    return new Decrypt();\n\n  this.key0 = 305419896;\n  this.key1 = 591751049;\n  this.key2 = 878082192;\n}\n\nDecrypt.prototype.update = function(h) {            \n  this.key0 = crc(h,this.key0);\n  this.key1 = bigInt(this.key0).and(255).and(4294967295).add(this.key1)\n  this.key1 = bigInt(this.key1).multiply(134775813).add(1).and(4294967295).value;\n  this.key2 = crc(bigInt(this.key1).shiftRight(24).and(255), this.key2);\n}\n\n\nDecrypt.prototype.decryptByte = function(c) {\n  var k = bigInt(this.key2).or(2);\n  c = c ^ bigInt(k).multiply(bigInt(k^1)).shiftRight(8).and(255);\n  this.update(c);\n  return c;\n};\n\n Decrypt.prototype.stream = function() {\n  var stream = Stream.Transform(),\n      self = this;\n\n  stream._transform = function(d,e,cb) {\n    for (var i = 0; i<d.length;i++) {\n      d[i] = self.decryptByte(d[i]);\n    }\n    this.push(d);\n    cb();\n  };\n  return stream;\n};\n\n\n\n\nmodule.exports = Decrypt;"]}